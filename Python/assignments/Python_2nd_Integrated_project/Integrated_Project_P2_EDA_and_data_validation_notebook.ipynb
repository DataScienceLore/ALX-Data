{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c542cfb",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Code_challenge.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f662d169",
   "metadata": {},
   "source": [
    "## Integrated project: Understanding and trusting data (Part 2)\n",
    "© ExploreAI Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26af890c",
   "metadata": {},
   "source": [
    "In this notebook, we're going to dive into the agricultural dataset to find patterns and try to get insights. We're also stopping for a moment to make sure our data are true and sound by validating it against other data sources. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d133774e",
   "metadata": {},
   "source": [
    "⚠️ **Note that this notebook guides us through the activities we need to complete in order to complete the MCQ. There will be 10 MCQs based on the analysis we complete, code we write, visuals we create, and insights we gain.** \n",
    "\n",
    "We could be asked to create code, modify code, or interpret some of the results, so we should take our time to do the analysis...\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Complete each step of the analysis.\n",
    "- Make notes after each output. Try to think about what the numbers say and what it means. \n",
    "- Explore the data fully, using all the tools we have learned so far.\n",
    "- Collaborate with your peers by discussing your insights and sharing your solutions. When it comes to the MCQ, be ready to take it on on your own!\n",
    "\n",
    "### MCQ checklist: \n",
    "- Ensure you have completed all code cells.\n",
    "- Make sure that you looked at and understood all outputs we discussed today.\n",
    "- Make sure you completed a crop-by-crop analysis of the data.\n",
    "- Make sure you took the time to get to know the dataset well.\n",
    "- Make sure you made thorough notes for each output.\n",
    "\n",
    "### What to expect in the MCQ\n",
    "- Questions on interpreting visuals.\n",
    "- Questions on interpreting data.\n",
    "- Questions to modify code and discuss/interpret data.\n",
    "- Questions on how to create visuals or perform analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8944ccbc",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2e633f",
   "metadata": {},
   "source": [
    "Fantastic progress so far! Now, let's roll up our sleeves and delve into the heart of our Maji Ndogo project – the exploratory data analysis (EDA) and visualisation. This phase is where our detective hats come on. We're going to sift through our dataset, turning numbers and stats into insightful, visual stories.\n",
    "\n",
    "Seaborn, our trusty Python visualisation library, will be our main tool here. Think of it as our magnifying glass, helping us zoom in on the nuances of our data. We'll create plots that show relationships between variables like rainfall, pH levels, and temperature. Scatter plots, line graphs, heatmaps – you name it, we'll be creating it. This isn't just about making pretty graphs; it’s about unearthing the hidden patterns and correlations that are key to our agricultural decisions.\n",
    "\n",
    "But wait, there's a critical step we can't overlook – validating our data. You see, our fields' data need a reality check. We'll be pulling in weather data from nearby stations to compare with our farm measurements. If the temperature, pH, and rainfall data from our farms align closely with these weather stations, we're on the right track. This isn't just about data integrity; it's about ensuring our data reflects the true environmental conditions of Maji Ndogo before we make conclusions or explore the data with AI.\n",
    "\n",
    "Here’s the task at hand: We'll use weather data from five stations across Maji Ndogo. Each field in our dataset is close to one of these stations, so we also have some data connecting each field in our dataset to the correct weather station. Our goal? To compare and validate key climate measurements. By confirming the accuracy of our data, we increase our confidence in the decisions we make next.\n",
    "\n",
    "This part of the journey is crucial. We're laying down a solid, trustworthy foundation for our project. Your keen eye for detail and analytical prowess will be invaluable here. Ready to dive into the depths of data validation and visual storytelling? Let's uncover the truths hidden in our numbers and bring precision to our farming strategies in Maji Ndogo.\n",
    "\n",
    "Onwards and upwards – let's make our data work for us and bring this vision to life!\n",
    "\n",
    "Saana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147be850",
   "metadata": {},
   "source": [
    "# Data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe8e55e",
   "metadata": {},
   "source": [
    "**1. Geographic features**\n",
    "\n",
    "- **Field_ID:** A unique identifier for each field (BigInt).\n",
    " \n",
    "- **Elevation:** The elevation of the field above sea level in metres (Float).\n",
    "\n",
    "- **Latitude:** Geographical latitude of the field in degrees (Float).\n",
    "\n",
    "- **Longitude:** Geographical longitude of the field in degrees (Float).\n",
    "\n",
    "- **Location:** Province the field is in (Text).\n",
    "\n",
    "- **Slope:** The slope of the land in the field (Float).\n",
    "\n",
    "**2. Weather features**\n",
    "\n",
    "- **Field_ID:** Corresponding field identifier (BigInt).\n",
    "\n",
    "- **Rainfall:** Amount of rainfall in the area in mm (Float).\n",
    "\n",
    "- **Min_temperature_C:** Average minimum temperature recorded in Celsius (Float).\n",
    "\n",
    "- **Max_temperature_C:** Average maximum temperature recorded in Celsius (Float).\n",
    "\n",
    "- **Ave_temps:** Average temperature in Celsius (Float).\n",
    "\n",
    "**3. Soil and crop features**\n",
    "\n",
    "- **Field_ID:** Corresponding field identifier (BigInt).\n",
    "\n",
    "- **Soil_fertility:** A measure of soil fertility where 0 is infertile soil and 1 is very fertile soil (Float).\n",
    "\n",
    "- **Soil_type:** Type of soil present in the field (Text).\n",
    "\n",
    "- **pH:** pH level of the soil, which is a measure of how acidic/basic the soil is (Float).\n",
    "\n",
    "**4. Farm management features**\n",
    "\n",
    "- **Field_ID:** Corresponding field identifier (BigInt).\n",
    "\n",
    "- **Pollution_level:** Level of pollution in the area where 0 is unpolluted and 1 is very polluted (Float).\n",
    "\n",
    "- **Plot_size:** Size of the plot in the field (Ha) (Float).\n",
    "\n",
    "- **Chosen_crop:** Type of crop chosen for cultivation (Text).\n",
    "\n",
    "- **Annual_yield:** Annual yield from the field (Float). This is the total output of the field. The field size and type of crop will affect the annual yield.\n",
    "\n",
    "- **Standard_yield:** Standardised yield expected from the field, normalised per crop (Float). This is independent of field size or crop type. Multiplying this number by the field size and average crop yield will give the Annual_yield."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936f0f12",
   "metadata": {},
   "source": [
    "**Weather_station_data (CSV)**\n",
    "\n",
    "- **Weather_station_ID:** The weather station the data originated from (Int).\n",
    "\n",
    "- **Message:** The weather data was captured by sensors at the stations, in the format of text messages (Str).\n",
    "\n",
    "**Weather_data_field_mapping (CSV)**\n",
    "\n",
    "- **Field_ID:** The id of the field that is connected to a weather station. This is the key we can use to join the weather station ID to the original data (Int).\n",
    "\n",
    "- **Weather_station_ID:** The weather station that is connected to a field. If a field has `weather_station_ID = 0` then that field is closest to weather station 0 (Int).\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c03f5c",
   "metadata": {},
   "source": [
    "# Importing and cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b39b1a",
   "metadata": {},
   "source": [
    "\n",
    "Alright, let's import the `Maji_Ndogo_farm_survey.db` file again from last time. Make sure to copy the `Maji_Ndogo_farm_survey.db` file into the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aedfcf44",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_engine, text \u001b[38;5;66;03m# Importing the SQL interface. If this fails, run !pip install sqlalchemy in another cell.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Create an engine for the database\u001b[39;00m\n\u001b[0;32m      8\u001b[0m engine \u001b[38;5;241m=\u001b[39m create_engine(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqlite:///Maji_Ndogo_farm_survey_small.db\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd # Importing the Pandas package with an alias, pd\n",
    "from sqlalchemy import create_engine, text # Importing the SQL interface. If this fails, run !pip install sqlalchemy in another cell.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Create an engine for the database\n",
    "engine = create_engine('sqlite:///Maji_Ndogo_farm_survey_small.db') #Make sure to have the .db file in the same directory as this notebook, and the file name matches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b71f19",
   "metadata": {},
   "source": [
    "Next up, we test if the connection works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ee695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as connection:\n",
    "    result = connection.execute(text(\"SELECT name FROM sqlite_master WHERE type='table';\"))\n",
    "    for row in result:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5838b83",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "\n",
    "`('geographic_features',)`\n",
    "\n",
    "`('weather_features',)`\n",
    "\n",
    "`('soil_and_crop_features',)`\n",
    "\n",
    "`('farm_management_features',)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70602482",
   "metadata": {},
   "source": [
    "And this time, I am using a better SQL query to import the data, so we don't have duplicate `Field_IDs` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305be5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "\"\"\"\n",
    "\n",
    "# Create a connection object\n",
    "with engine.connect() as connection:\n",
    "    \n",
    "    # Use Pandas to execute the query and store the result in a DataFrame\n",
    "    MD_agric_df = pd.read_sql_query(text(sql_query), connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02566073",
   "metadata": {},
   "source": [
    "Cleaning up the data like last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e639d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "MD_agric_df.rename(columns={'Annual_yield': 'Crop_type_Temp', 'Crop_type': 'Annual_yield'}, inplace=True)\n",
    "MD_agric_df.rename(columns={'Crop_type_Temp': 'Crop_type'}, inplace=True)\n",
    "MD_agric_df['Elevation'] = MD_agric_df['Elevation'].abs()\n",
    "\n",
    "# Correcting 'Crop_type' column\n",
    "def correct_crop_type(crop):\n",
    "    crop = crop.strip()  # Remove trailing spaces\n",
    "    corrections = {\n",
    "        'cassaval': 'cassava',\n",
    "        'wheatn': 'wheat',\n",
    "        'teaa': 'tea'\n",
    "    }\n",
    "    return corrections.get(crop, crop)  # Get the corrected crop type, or return the original if not in corrections\n",
    "\n",
    "# Apply the correction function to the Crop_type column\n",
    "MD_agric_df['Crop_type'] = MD_agric_df['Crop_type'].apply(correct_crop_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fee0a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "MD_agric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b932ad",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89abdf56",
   "metadata": {},
   "source": [
    "Exploratory data analysis (EDA) is a crucial step in the data science process. The main goals are to understand:\n",
    "1. What the variables or **feature** variables in our dataset mean.\n",
    "2. What the distributions of those feature variables are through univariate analysis.\n",
    "3. What the relationships are between the feature variables, and what the relationships are between the feature variables and our target variable `Standard_yield`. We do this by doing a multivariate analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f9e24e",
   "metadata": {},
   "source": [
    "## Univariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d724452e",
   "metadata": {},
   "source": [
    "The first step once our data are cleaned is to look at the dataset as a whole using methods like `df.info()` or `df.describe()`. \n",
    "\n",
    "**Task**: Use `df.info()` or `df.describe()` to understand the data we have access to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bec8349",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f4d631",
   "metadata": {},
   "source": [
    "Ok, first of all, does describe on `Field_ID` tell us anything useful? What about the `Crop_type`? Why isn't the mean of the crop types shown? Can we take the mean of a categorical variable? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f65ace1",
   "metadata": {},
   "source": [
    "It is really important that we think about the meaning of numbers. If we focus purely on getting an answer, instead of whether it makes sense given the context and what we know about a variable, we’ll often “find” relationships or patterns that are purely coincidental. The mean of `Field_ID` is an example of something that doesn’t actually make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67489111",
   "metadata": {},
   "source": [
    "**Make some notes** about the columns and what the **story** is that `.describe()` is telling us:\n",
    "\n",
    "- **Elevation** – I see... It means that...\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6293cd",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "Next up, let's have a look at the distribution of data in our columns. To do this, we need to use `Seaborn`. Add it to our package imports and create KDE distribution plots of each numeric column. \n",
    "\n",
    "\n",
    "To plot multiple visualisations in a single figure in `matplotlib`, we can create a plot with subplots.\n",
    "\n",
    "⚠️ **Note:** If the multiple plot figure code is too complex, simply plot each column's distribution and make some notes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2d725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a 5x4 grid of plots\n",
    "fig, axes = plt.subplots(4, 4, figsize=(15, 15))  # Adjust the figure size as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac46c785",
   "metadata": {},
   "source": [
    "This creates plots we can access by `axes[i][j]`. This is pretty hard to iterate through, so we can use a command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffbce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = axes.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800caeaa",
   "metadata": {},
   "source": [
    "This reformats axes to only require one index, `axes[i]`, from 0 to 15 (for each of the 16 plots).\n",
    "Now we can create a for loop, for i in range(0,15)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f622f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,15):\n",
    "    axes[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332882e6",
   "metadata": {},
   "source": [
    "To plot all of the columns, we can use `enumerate()` over the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33985cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, column in enumerate(numerical_columns.columns):\n",
    "    # sns.kdeplot(..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6809249a",
   "metadata": {},
   "source": [
    "For each column, we have an `i` value for the plot axes, and a `column` we can use to plot with. Another tip: We may want to see the distribution of the data in relation to the mean value of that column too – we can add these two lines to plot the mean value as a striped, red, vertical line: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1034a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_val = MD_agric_df[column].mean()\n",
    "axes[i].axvline(mean_val, color='red', linestyle='dashed', linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712aa070",
   "metadata": {},
   "source": [
    "**Task:** Create a single image plot with KDE plots of each column's distribution by completing the code `sns.kdeplot(...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366db3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create either a single image of KDE plots for each numerical column, or plot each KDE one by one. \n",
    "\n",
    "numerical_columns = MD_agric_df.drop(columns=[\"Field_ID\"])\n",
    "numerical_columns = numerical_columns.select_dtypes('number')\n",
    "# Assuming MD_agric_df is already defined with the simulated data\n",
    "\n",
    "# Setting up a 5x4 grid of plots\n",
    "fig, axes = plt.subplots(4, 4, figsize=(15, 15))  # Adjust the figure size as needed\n",
    "axes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\n",
    "\n",
    "# Plotting a KDE for each column in its respective subplot\n",
    "for i, column in enumerate(numerical_columns.columns):\n",
    "    # sns.kdeplot(...\n",
    "\n",
    "# ANSWER\n",
    "# Create either a single image of KDE plots for each numerical column, or plot each KDE one by one. \n",
    "\n",
    "numerical_columns = MD_agric_df.drop(columns=[\"Field_ID\"])\n",
    "numerical_columns = numerical_columns.select_dtypes('number')\n",
    "# Assuming MD_agric_df is already defined with the simulated data\n",
    "\n",
    "# Setting up a 5x4 grid of plots\n",
    "fig, axes = plt.subplots(4, 4, figsize=(15, 15))  # Adjust the figure size as needed\n",
    "axes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\n",
    "\n",
    "# Plotting a KDE for each column in its respective subplot\n",
    "for i, column in enumerate(numerical_columns.columns):\n",
    "    \n",
    "    sns.kdeplot(MD_agric_df[column], fill=True, ax=axes[i])\n",
    "    axes[i].set_title(f'KDE of {column}')\n",
    "    axes[i].set_xlabel(column)\n",
    "    axes[i].set_ylabel('Density')\n",
    "    mean_val = MD_agric_df[column].mean()\n",
    "    axes[i].axvline(mean_val, color='red', linestyle='dashed', linewidth=2)\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731f0118",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "- The distribution of the `Slope` variable is skewed a bit to the left, which means the mean value may not be the best measure of central tendency. Most values are below the mean, but there are some extreme values influencing the mean calculation. We should be careful when we use this column in statistical calculations. \n",
    "\n",
    "- The KDE of `Rainfall` appears normal but seems to have multiple peaks. This may indicate underlying patterns that are overlapping. We should take a closer look. \n",
    "\n",
    "- The distribution of... which means that...\n",
    "\n",
    "-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48d7002",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's have a closer look at the `Rainfall` data distribution. We can use the `hue` function in `Seaborn` to separate the data by a category. Let's try segregating the data by `Location` and `Crop_type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e37a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a KDE plot of the Rainfall column using hue.\n",
    "sns.kdeplot(data = MD_agric_df, x = 'Rainfall', hue= 'Crop_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeb6ad6",
   "metadata": {},
   "source": [
    "**Notes:** \n",
    "\n",
    "**Location** \n",
    "- When we split the data by `Location` I see... which means...\n",
    "- \n",
    "**Crop type** \n",
    "- When we split the data by `Crop_type` I see... which means...\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835f7f6b",
   "metadata": {},
   "source": [
    "Answer some of these questions:\n",
    "Does it rain more in Kilimani than Akatsi? \n",
    "Does it rain more in Kilimani than Amanzi?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baabd36",
   "metadata": {},
   "source": [
    "All the answers lie in the x-axis alone. Akatsi, on average, has a higher rainfall number than Kilimani, and Amanzi's average rainfall is quite similar to Kilimani, so there is no difference really. We can confirm this by grouping our data by `Location` and calculating the means of the `Rainfall` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cbd3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean rainfall in each province.\n",
    "MD_agric_df.groupby('Location').mean(numeric_only = True)['Rainfall']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488c43be",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "\n",
    " - Amanzi is the province with the lowest average rainfall. Potatoes and maize seem to grow in lower rainfall regions. Is there a connection?\n",
    "\n",
    " - Sokoto has ... which means that...\n",
    " -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673152e5",
   "metadata": {},
   "source": [
    "Note that the height of KDE plots is much harder to interpret. It requires a good understanding of what the term `Density` means in this context and is covered well in `Seaborn`'s documentation.\n",
    "\n",
    "Repeat this analysis for other distributions that look like there are multiple peaks, try to identify more patterns, and try to explain those patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0471c7b2",
   "metadata": {},
   "source": [
    "## Multivariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fa08d7",
   "metadata": {},
   "source": [
    "In the last step we touched on some multivariate analysis when we noticed that `Location` and `Rainfall` are connected, and that `Rainfall` and `Crop_type` are connected, so let's dive in deeper. Multivariate analysis is all about understanding the relationships our variables have. We can have relationships between numerical columns, numerical and categorical columns, and even between categorical columns, so let's take a look at how we handle each of these cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf902b3",
   "metadata": {},
   "source": [
    "## Categorical and continuous relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733d21bc",
   "metadata": {},
   "source": [
    "One of the tools we can use is a Violin plot that is particularly useful for comparing the distribution of a continuous variable across different levels of a categorical variable. It is similar to the KDE plot, but the distributions are split apart a bit so it is simpler to understand. Note though, often our audience has never encountered a Violin plot before, so make sure to explain what it is and how to interpret it when you present these visualisations. \n",
    "\n",
    "**Task:** Create a Violin plot of `Rainfall` distributions for various crop types by completing the code `sns...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b1b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can change the dimensions of our plot with this line of code. Make sure to choose a size that highlights the features of the data.\n",
    "plt.figure( figsize =(5 ,10))\n",
    "\n",
    "#Plot a Violin plot\n",
    "sns...\n",
    "\n",
    "\n",
    "# ANSWER\n",
    "# We can change the dimensions of our plot with this line of code. Make sure to choose a size that highlights the features of the data.\n",
    "plt.figure( figsize =(5 ,10))\n",
    "\n",
    "#Plot a Violin plot\n",
    "sns.violinplot(data = MD_agric_df, x = 'Rainfall', y= \"Crop_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffb2d9c",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    " - Rice seems to grow mostly where there is about 1600 mm of annual rainfall, while coffee can grow across a wide range of rainfall conditions. Does that mean coffee is a more resilient crop than rice?\n",
    " \n",
    " - Bananas seem to prefer... so...\n",
    " - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5830b2e",
   "metadata": {},
   "source": [
    "Other options to visualise categorical/continuous data are scatter plots, FacetGrids, or Bubble charts. Be adventurous and try one of these!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638d38c4",
   "metadata": {},
   "source": [
    "## Continuous relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388eafea",
   "metadata": {},
   "source": [
    "The quickest way to visualise smaller datasets, like this one, is to use a `pairplot()`. Create a pairplot of the dataset that colours the data based on which crop type it is.\n",
    "\n",
    "Note that pairplots take a huge amount of resources, so it may take a while to display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51f8d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_columns =['Field_ID','Latitude','Longitude','Annual_yield']\n",
    "\n",
    "sns.pairplot(MD_agric_df.drop(columns =remove_columns), hue=\"Crop_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a18a220",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    " - There are some linear relationships like `Elevation` vs. `Min_temperature_C`. This means...\n",
    "\n",
    " - It seems like the crop types are often clustered together. In a plot of `Standard_yield` vs. `pH` there is a large blob of tea data between `pH` 4 and 6 and `Standard_yield` between 0.6 and 0.8. This means...\n",
    " \n",
    " - Most relationships have some patterns to them, but it is too complex to identify. For example, `Min_temperature_C` and `Rainfall` seem to be linear but spread apart. The data looks \"streaky\" so there may be a pattern hidden.\n",
    " \n",
    " - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c53ef92",
   "metadata": {},
   "source": [
    "## Categorical relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42910e75",
   "metadata": {},
   "source": [
    "Categorical relationships are also useful to understand. To analyse these we can use a Pandas tool called `crosstab()`. `crosstab()` checks the number of times categorical features co-exist. For example, we saw earlier that Amanzi is the province with the lowest rainfall, and we also saw that most of the maize and potatoes are planted in low-rainfall areas. If we look at the cross tab of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90211c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(MD_agric_df['Location'],MD_agric_df['Crop_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc61589c",
   "metadata": {},
   "source": [
    "From this `crosstab()` we can see that when `Location` is Amanzi, the counts of each field with a specific `Crop_type` are shown. For Amanzi, potatoes, wheat, and maize occur a lot more frequently than the other crops, and as we saw earlier, this is because Amanzi has less rainfall, making these crops more viable. What other relationships can we get from this table?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78b6437",
   "metadata": {},
   "source": [
    "**Notes:** \n",
    "\n",
    "- When The `Location` is `Rural_Kilimani`, potatoes are the most frequent crops, followed by wheat. \n",
    "\n",
    "- When the `Location` is `Rural_Sokoto`...\n",
    "\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af605200",
   "metadata": {},
   "source": [
    "## Yield-related relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9c1382",
   "metadata": {},
   "source": [
    "We have found many interesting things in the data looking at the distributions and relationships between variables, but the question we actually have is: **What affects the `Standard_yield`**? Do all crops do better in high-rainfall places? Do all crops grow better on flat terrain where the slope is low? \n",
    "\n",
    "To answer this we can look at the last row of the pairplot we made earlier, or we could look at a correlation matrix. \n",
    "\n",
    "In Pandas we can easily check if variables are linearly correlated using the `df.corr()` method. It prints out a table containing the Pearson correlation coefficient of each variable with every other variable. A value close to 1 shows us that the variables are perfectly correlated, and a value of -1 shows us that variables are inversely correlated. A value close to 0 means that there is no correlation between the variables, but it does not mean there is no relationship. They could still be correlated through a non-linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce617365",
   "metadata": {},
   "outputs": [],
   "source": [
    "MD_agric_df.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f68fd2",
   "metadata": {},
   "source": [
    "Since we're only interested in the `Standard_yield`, it will be useful to only look at the `Standard_yield` column and sort the values. \n",
    "\n",
    "**Task:** Create a sorted list of correlation coefficients for `Standard_yield` only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8860aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "\n",
    "# ANSWER\n",
    "MD_agric_df.corr(numeric_only=True)['Standard_yield'].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b709ab7d",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "\n",
    "- `Pollution_level` is inversely correlated (weak) with `Standard_yield`, so when places are polluted, crops produce less.\n",
    "\n",
    "- `Min_temperature_C` is weakly correlated with `Standard_yield`. So when the minimum temperature is higher, crops produce more. So when it doesn't get too cold, crops grow better.\n",
    "\n",
    "- No single feature can explain why a crop does well. There are many weak correlations and correlations only look at linear relationships, so features like `Rainfall` that have a low correlation may just not be linearly correlated. \n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c522a12e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The analysis so far is very basic. We're looking at the big picture and trying to understand how all of these features interact with one another to produce good crop yields. We can note, though, that it is complicated. Many things affect these crops. \n",
    "\n",
    "It may be beneficial to look at each crop on its own to understand how features interact with a single crop. \n",
    "\n",
    "Let's take a closer look at coffee, create a DataFrame that only contains the `coffee` crop type, and analyse the data to understand what makes a coffee crop successful. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fdad36",
   "metadata": {},
   "source": [
    "### Coffee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117063e6",
   "metadata": {},
   "source": [
    "First, we create a DataFrame for only coffee crops, and then look at the pairplot of this DataFrame to understand what affects the coffee crop. By doing this we can remove some of the complexity brought by the different crop types. Each crop type may interact differently with rainfall, pH, and soil type, so by examining each crop category closely we can hopefully gain a bit deeper insight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48f7058",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_df = MD_agric_df.query(\"Crop_type == 'coffee'\")\n",
    "coffee_df = coffee_df.drop(columns = ['Crop_type','Field_ID','Annual_yield'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebc9136",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(coffee_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab66c5c2",
   "metadata": {},
   "source": [
    "**Notes:** \n",
    "\n",
    "- Coffee crop yield has a positive correlation with rainfall; crop yields are best when there is a lot of rain. \n",
    "\n",
    "- Coffee crop yield is higher when the soil is more fertile, so it seems coffee benefits a lot from rich soil. \n",
    "\n",
    "- Highly polluted areas lower the crop output of coffee. Pollution has a significant effect on the crop yield.\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a98a814",
   "metadata": {},
   "source": [
    "### Other crop types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f61835a",
   "metadata": {},
   "source": [
    "**Task:** Analyse the data of other crop types and try to form some understanding of what makes it successful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b985a37",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9e016f",
   "metadata": {},
   "source": [
    "Ok, now take some time to look at all of the results we have in a bigger picture. For example, one of the big takeaways from this analysis is that crops tend to be planted in places where they do well, but not always. Some crops prefer lower rainfall, and are therefore doing well in places with lower rainfall. \n",
    "\n",
    "Answer questions like, what makes tea grow well? \n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b042e1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "# Weather data validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381c984d",
   "metadata": {},
   "source": [
    "## Introduction to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880f960d",
   "metadata": {},
   "source": [
    "Ok, so before we dive any deeper into our data, it is a good idea to think about data quality a bit. How do we know the data are accurate and, as a whole, reflect the reality in Maji Ndogo? \n",
    "\n",
    "It is critical to be sceptical of the data and the things we learn from it. So, let's validate a portion of our data with another data source. \n",
    "\n",
    "There are weather stations scattered across Maji Ndogo that monitor weather-related data in real-time through Internet of Things (IoT) devices. These sensors send their data to a central database. Ideally, the data we get is cleaned, but in this case, we can only access the raw data these sensors bring in, so we have a bit of cleaning to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc25c217",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "    <img src=\"https://github.com/Explore-AI/Pictures/blob/master/alx_ds_python/Code_challenges/Weather_station_0.png?raw=true\" style=\"display: block; margin-left: auto; margin-right: auto; width: 50%;\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f52eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_df = pd.read_csv(\"Weather_station_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e26546",
   "metadata": {},
   "source": [
    "If we have a look at the data, we can see that there are text messages and data about which weather station it came from. Let's have a look at some of these messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90babe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None) # This line will remove column width limits so we can read the contents\n",
    "\n",
    "weather_station_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a13c11",
   "metadata": {},
   "source": [
    "Let's have a look at some of these messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80af6e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "weather_station_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395f1171",
   "metadata": {},
   "source": [
    "The messages all contain a timestamp, which doesn't matter in this analysis, a description of what the measurement was, and the value of that measurement. There are messages about temperatures, rainfall, and air quality. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf41613",
   "metadata": {},
   "source": [
    "But the problem is that the messages are not formatted in a specific way. \n",
    "\n",
    "If we look at temperature-related messages, there are two different messages, `Temp. Reading [2023-05-23 09:41:36]: Current 14.53 C.`, `Temperature Read at [2022-09-29 08:22:45]: 13.18C.` and maybe even more. \n",
    "\n",
    "We're going to have to use some string methods to get the values and also classify them as rainfall, temperature, or pollution data. \n",
    "\n",
    "<br> \n",
    "\n",
    "Before we dive in, let's think through what we're trying to do. \n",
    "\n",
    "Our main dataset has weather-related data, `Ave_temps`, `Rainfall`, and `Pollution_level` features. If we take the average of these values, we can compare it with the averages of the weather stations. \n",
    "\n",
    "The catch is: Which fields do we compare to which station's data? The data below links each field in our main dataset to a weather station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a561b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_mapping_df = pd.read_csv(\"Weather_data_field_mapping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2229acdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_mapping_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7786969b",
   "metadata": {},
   "source": [
    "## The plan of action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac80efa",
   "metadata": {},
   "source": [
    "So here's the plan.\n",
    "\n",
    "1. We need to extract all of the measurement types and values from the weather station data. It is important that we check if each message's data are extracted properly before we move on. \n",
    "\n",
    "2. Take the average value of each measurement type from the weather data.\n",
    "\n",
    "3. Join the `weather_station_mapping_df` to `MD_agric_df`, and take the averages of the weather data per station.\n",
    "\n",
    "4. Compare the averages from the weather data to the averages of the main dataset to confirm whether our data agrees reasonably well with the weather data. \n",
    "\n",
    "Think for a second why this could work. If we assume that on average, the farms in a region should experience the same weather as a station somewhere in that region, then their averages should be the same. If the averages do match, we can be pretty confident that the measurements made at the farms represent the reality on the ground, since our independent measurements of weather agreed. \n",
    "\n",
    "Hopefully this is the case, so let's get started on extracting the messages. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33070d6",
   "metadata": {},
   "source": [
    "## Data extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49579700",
   "metadata": {},
   "source": [
    "Each of the measurement types will likely have a couple of different ways the data are structured, so we need to find all the different ways messages can be sent, identify patterns in the messages, and use regex to retrieve the value. We'll take on the temperature data together, and then you can complete the rest.\n",
    "\n",
    "First up, let's look at the messages and find the temperature-related messages first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b45428",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f08e1f",
   "metadata": {},
   "source": [
    "Ok, so we can see that there are messages:\n",
    "1. `【2022-01-04 21:47:48】温度感应: 现在温度是 12.82C.` – 温度感应 seems to be related to temperature, and the `C` at the end confirms this message is related to temperature.\n",
    "\n",
    "2. `Temp. Reading [2023-05-23 09:41:36]: Current 14.53 C.` – This one is straightforward, but note that this message has a space between the value and the `C`.\n",
    "\n",
    "3. `Temperature Read at [2022-01-08 02:54:10]: 12.75C.` – This one has no space between the C and the value again. \n",
    "\n",
    "There may be more, but we will build the regex on these three cases and check at the end if we missed any. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e817c3",
   "metadata": {},
   "source": [
    "Given these variations, we need a regex pattern that can flexibly capture the temperature value in each case. Here’s how we break it down:\n",
    "\n",
    "`(\\d+(\\.\\d+)?)`: This is the core part of our pattern. `\\d+` matches one or more digits, capturing whole numbers. The `(\\.\\d+)?` part captures decimal points followed by one or more digits – the `?` makes this decimal part optional, allowing us to match both whole numbers and decimals.\n",
    "\n",
    "`\\s?C`: Next, we account for the possible space (or absence thereof) before the `C`. `\\s?` matches zero or one whitespace character – the `?` denotes that the space may or may not be present. Finally, `C` literally matches the character `'C'`.\n",
    "\n",
    "When we put it all together – `(\\d+(\\.\\d+)?)\\s?C` – we get a pattern that will match a number (either whole or decimal), optionally followed by a space, and then a `C`. This pattern covers all our cases, ensuring we accurately extract temperature values from each message type.\n",
    "\n",
    "Using that regex pattern, we can extract the value, but we also need to classify this as a `Temperature` measurement. We can create a dictionary called `patterns` to hold this pattern and the patterns for the other measurements. The keys will classify the pattern as `Temperature` and the values are the regex pattern we want to match to. Our logic should basically be to check if the `Message` column matches this pattern. If it does, save the key to a `Measurement` column and the value to `Measurement_value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf952808",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = {\n",
    "        'Temperature': r'(\\d+(\\.\\d+)?)\\s?C'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50ec1b8",
   "metadata": {},
   "source": [
    "Let's put this in a function and apply the function to the DataFrame. `extract_measurement` will create a Pandas Series with a tuple inside `(measurement_type,value)`, for example, `('Temperature', 15.6)`. We then use lambda expressions to unpack that tuple into two separate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1751526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # Importing the regex pattern\n",
    "import numpy as np\n",
    "weather_station_df = pd.read_csv(\"Weather_station_data.csv\")\n",
    "\n",
    "def extract_measurement(message):\n",
    "    \"\"\"\n",
    "    Extracts a numeric measurement value from a given message string.\n",
    "\n",
    "    The function applies regular expressions to identify and extract\n",
    "    numeric values related to different types of measurements such as\n",
    "    Rainfall, Average Temperatures, and Pollution Levels from a text message.\n",
    "    It returns the key of the matching record, and first matching value as a floating-point number.\n",
    "    \n",
    "    Parameters:\n",
    "    message (str): A string message containing the measurement information.\n",
    "\n",
    "    Returns:\n",
    "    float: The extracted numeric value of the measurement if a match is found;\n",
    "           otherwise, None.\n",
    "\n",
    "    The function uses the following patterns for extraction:\n",
    "    - Rainfall: Matches numbers (including decimal) followed by 'mm', optionally spaced.\n",
    "    - Ave_temps: Matches numbers (including decimal) followed by 'C', optionally spaced.\n",
    "    - Pollution_level: Matches numbers (including decimal) following 'Pollution at' or '='.\n",
    "    \n",
    "    Example usage:\n",
    "    extract_measurement(\"【2022-01-04 21:47:48】温度感应: 现在温度是 12.82C.\")\n",
    "    # Returns: 'Temperature', 12.82\n",
    "    \"\"\"\n",
    "    \n",
    "    for key, pattern in patterns.items(): # Loop through all of the patterns and check if they match the pattern value.\n",
    "        match = re.search(pattern, message)\n",
    "        if match:\n",
    "            # Extract the first group that matches, which should be the measurement value if all previous matches are empty.\n",
    "            # print(match.groups()) # Uncomment this line to help you debug your regex patterns.\n",
    "            return key, float(next((x for x in match.groups() if x is not None)))\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# The function creates a tuple with the measurement type and value into a Pandas Series\n",
    "result = weather_station_df['Message'].apply(extract_measurement)\n",
    "\n",
    "# Create separate columns for 'Measurement' and 'extracted_value' by unpacking the tuple with Lambda functions.\n",
    "weather_station_df['Measurement'] = result.apply(lambda x: x[0])\n",
    "weather_station_df['Value'] = result.apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb1ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766059d5",
   "metadata": {},
   "source": [
    "We can see that the temperature-related measurements are now correctly added. \n",
    "\n",
    "Take a moment to look for `Values` like `2022`, `31`, `01`, which may indicate that the regex patterns we used are capturing the timestamp information. \n",
    "\n",
    "\n",
    "\n",
    "**Task:** Create regex patterns for the other two measurement types by modifying `patterns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c3c877",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = {\n",
    "    \n",
    "    'Temperature': r'(\\d+(\\.\\d+)?)\\s?C',\n",
    "    'Rainfall': # Insert your regex here\n",
    "    'Pollution_level': # Insert your regex here\n",
    "    }\n",
    "\n",
    "# ANSWER: \n",
    "patterns = {\n",
    "    'Rainfall': r'(\\d+(\\.\\d+)?)\\s?mm',\n",
    "     'Temperature': r'(\\d+(\\.\\d+)?)\\s?C',\n",
    "    'Pollution_level': r'=\\s*(-?\\d+(\\.\\d+)?)|Pollution at \\s*(-?\\d+(\\.\\d+)?)'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24931811",
   "metadata": {},
   "source": [
    "Search using that pattern, and create the appropriate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7121a01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function creates a tuple with the measurement type and value into a Pandas Series\n",
    "result = weather_station_df['Message'].apply(extract_measurement)\n",
    "\n",
    "# Create separate columns for 'Measurement' and 'extracted_value' by unpacking the tuple with Lambda functions.\n",
    "weather_station_df['Measurement'] = result.apply(lambda x: x[0])\n",
    "weather_station_df['Value'] = result.apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb3ec55",
   "metadata": {},
   "source": [
    "We can test if we were able to extract values from all the messages if the following code returns an empty DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2f47b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this line of code to see which messages are not assigned yet.\n",
    "weather_station_df[(weather_station_df['Measurement'] == None)|(weather_station_df['Value'].isna())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1294cd",
   "metadata": {},
   "source": [
    "## Comparing means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46287d63",
   "metadata": {},
   "source": [
    "### Weather station data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b3b190",
   "metadata": {},
   "source": [
    "Great, now that our data have been cleaned up, we can calculate the mean `Temperature`, `Rainfall`, and `Pollution_level` for **each** weather station from the weather data. \n",
    "\n",
    "**Task:** Calculate the mean `Temperature`, `Rainfall`, and `Pollution_level` by weather station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7a40eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_means = weather_station_df.groupby(<INSERT YOUR CODE HERE>)['Value'].mean(numeric_only = True)\n",
    "weather_station_means = weather_station_means.unstack()\n",
    "weather_station_means\n",
    "\n",
    "#ANSWER\n",
    "weather_station_means = weather_station_df.groupby(by = ['Weather_station_ID','Measurement'])['Value'].mean(numeric_only = True)\n",
    "weather_station_means = weather_station_means.unstack()\n",
    "weather_station_means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575fe31c",
   "metadata": {},
   "source": [
    "Expected values: \n",
    "```python \n",
    "Measurement\t    Pollution_level\tRainfall\tTemperature\n",
    "Weather_station_ID\t\t\t\n",
    "0\t            0.352791\t        1575.953750\t13.403900\n",
    "1\t            0.257333\t        577.383910\t12.998899\n",
    "2\t            0.043858\t        1690.955324\t13.179717\n",
    "3\t            0.238190\t        905.191397\t13.256779\n",
    "4\t            0.128261\t        1200.183505\t13.188571\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9161215",
   "metadata": {},
   "source": [
    "Now we have a DataFrame where we can retrieve the mean weather measurement values of any weather station using indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6df0dbe",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "### Main dataset preparation\n",
    "\n",
    "Now we need to calculate the mean weather-related data per weather station from our **main dataset**, so we need to merge the mapping data first. \n",
    "\n",
    "Merge `MD_agric_df` with `weather_station_mapping_df` to create a `Weather_station_ID` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2cff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MD_agric_df columns\")\n",
    "print(MD_agric_df.columns)\n",
    "print(\"\\n\")\n",
    "print(\"weather_station_mapping_df columns\")\n",
    "print(weather_station_mapping_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0f5493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "\n",
    "\n",
    "# ANSWER\n",
    "\n",
    "MD_agric_df = MD_agric_df.merge(weather_station_mapping_df,on = 'Field_ID', how='left')\n",
    "MD_agric_df.drop(columns=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085799e5",
   "metadata": {},
   "source": [
    "Your DataFrame should have **5654 rows × 19 columns**. Make sure to remove duplicate or Unnamed columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30630dc",
   "metadata": {},
   "source": [
    "Then calculate the mean values per weather station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe0912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MD_agric_df_weather_means = MD_agric_df.groupby(\"Weather_station\").mean(numeric_only = True)[['Pollution_level','Rainfall', 'Ave_temps']]\n",
    "\n",
    "MD_agric_df_weather_means = MD_agric_df_weather_means.rename(columns = {'Ave_temps':\"Temperature\"})\n",
    "MD_agric_df_weather_means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832d1614",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "```python\n",
    "\t\t\tPollution_level\tRainfall\tAve_temps\n",
    "Weather_station\t\t\t\n",
    "0\t\t\t0.367706\t1522.894473\t13.393418\n",
    "1\t\t\t0.244627\t568.445319\t13.058085\n",
    "2\t\t\t0.043810\t1705.834280\t13.168364\n",
    "3\t\t\t0.271464\t931.607860\t13.224666\n",
    "4\t\t\t0.139913\t1168.991800\t13.19799\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a83440",
   "metadata": {},
   "source": [
    "### Comparing our datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e512dd",
   "metadata": {},
   "source": [
    "If the format of your `MD_agric_df_weather_means` and `weather_station_means` are correct, then running the code below will check each mean, compare it to the weather data, and print out whether it is within 1.5% of one another.\n",
    "\n",
    "If you are feeling brave, code out your own solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c167762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def within_tolerance_percentage(extracted, original, tolerance_pct):\n",
    "    \"\"\"\n",
    "    Purpose: This function calculates the percentage difference between two values (extracted and original) and checks if this difference is within a specified tolerance percentage (tolerance_pct).\n",
    "    \n",
    "    Parameters:\n",
    "    extracted: A numerical value representing the extracted or observed data point.\n",
    "    original: A numerical value representing the original or expected data point.\n",
    "    tolerance_pct: A numerical value representing the tolerance limit as a percentage.\n",
    "    \n",
    "    Returns: A Boolean value (True or False). It returns True if the percentage difference between extracted and original is less than or equal to tolerance_pct; otherwise, it returns False.\n",
    "    \n",
    "    Calculation: The percentage difference is calculated as abs((extracted - original) / original) * 100.\n",
    "    \"\"\"\n",
    "    # Calculate the percentage difference\n",
    "    percent_diff = abs((extracted - original) / original) * 100\n",
    "    # Check if within tolerance\n",
    "    return percent_diff <= tolerance_pct\n",
    "\n",
    "\n",
    "\n",
    "def check_means(MD_agric_df_weather_means, weather_station_means):\n",
    "    \"\"\"\"\n",
    "    Purpose: This function iterates over two data frames (MD_agric_df_weather_means and weather_station_means), compares the mean values of corresponding entries, and checks if they are within a specified tolerance range using the within_tolerance_percentage function.\n",
    "\n",
    "    Parameters: \n",
    "    MD_agric_df_weather_means: A Pandas DataFrame containing original mean values for various measurements, indexed by weather station IDs.\n",
    "    weather_station_means: A Pandas DataFrame containing extracted mean values for various measurements, indexed by weather station IDs.\n",
    "\n",
    "    Functionality: \n",
    "    Iterates through each row in weather_station_means. \n",
    "    For each row, iterates through each measurement. \n",
    "    Retrieves the extracted_mean from weather_station_means and the corresponding original_mean from MD_agric_df_weather_means.\n",
    "    Uses the within_tolerance_percentage function to check if the extracted_mean is within the specified tolerance percentage of the original_mean.\n",
    "    Keeps count of how many measurements are within and outside the tolerance range (true_count and false_count respectively).\n",
    "    Prints detailed information for each measurement, including the weather station ID, measurement name, extracted mean, original mean, and whether it is within specification.\n",
    "\n",
    "    Outputs: At the end, the function prints the total count of measurements that are within (True) and outside (False) the tolerance range.\n",
    "\n",
    "    Note: The function assumes the existence of a global variable tolerance_pct that specifies the tolerance percentage.\n",
    "    These functions appear to be particularly useful in data validation processes, where comparing data sets and ensuring their consistency within certain limits is crucial.\n",
    "    \"\"\"\n",
    "    true_count = 0\n",
    "    false_count = 0\n",
    "    for index, row in weather_station_means.iterrows():\n",
    "\n",
    "        print(f\"Weather Station ID: {index}\")\n",
    "        for measurement in row.index:\n",
    "            print (measurement)\n",
    "            extracted_mean = row[measurement]\n",
    "            original_mean = MD_agric_df_weather_means.loc[index, measurement]\n",
    "            within_spec = within_tolerance_percentage(extracted_mean, original_mean, tolerance_pct)\n",
    "            if  within_spec == True:\n",
    "                true_count +=1\n",
    "            else:\n",
    "                false_count +=1\n",
    "            print(f\"  Measurement: {measurement}, Extracted Mean: {extracted_mean}, Original Mean: {original_mean}, Within Spec: {within_spec}\")\n",
    "            print(\" \")\n",
    "    print(f\"True: {true_count}, False: {false_count}\")\n",
    "\n",
    "\n",
    "\n",
    "# Example tolerance percentage (e.g., 5%)\n",
    "tolerance_pct = 1.5\n",
    "\n",
    "check_means(MD_agric_df_weather_means, weather_station_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3546ac",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```python\n",
    "Weather Station ID: 0\n",
    "Pollution_level\n",
    "  Measurement: Pollution_level, Extracted Mean: 0.35279069767441856, Original Mean: 0.36770623143001924, Within Spec: False\n",
    " \n",
    "Rainfall\n",
    "  Measurement: Rainfall, Extracted Mean: 1575.9537500000001, Original Mean: 1522.8944727272726, Within Spec: False\n",
    " \n",
    "Temperature\n",
    "  Measurement: Temperature, Extracted Mean: 13.4039, Original Mean: 13.393418181818182, Within Spec: True\n",
    " \n",
    "Weather Station ID: 1\n",
    "Pollution_level\n",
    "  Measurement: Pollution_level, Extracted Mean: 0.2573333333333333, Original Mean: 0.24462714245217726, Within Spec: False\n",
    " \n",
    "Rainfall\n",
    "  Measurement: Rainfall, Extracted Mean: 577.3839097744361, Original Mean: 568.4453191489362, Within Spec: False\n",
    " \n",
    "Temperature\n",
    "  Measurement: Temperature, Extracted Mean: 12.998899082568807, Original Mean: 13.058085106382979, Within Spec: True\n",
    " \n",
    "Weather Station ID: 2\n",
    "Pollution_level\n",
    "  Measurement: Pollution_level, Extracted Mean: 0.043858267716535435, Original Mean: 0.043809732976173035, Within Spec: True\n",
    " \n",
    "Rainfall\n",
    "  Measurement: Rainfall, Extracted Mean: 1690.9553237410073, Original Mean: 1705.834280117532, Within Spec: True\n",
    " \n",
    "Temperature\n",
    "  Measurement: Temperature, Extracted Mean: 13.179716981132074, Original Mean: 13.168364348677766, Within Spec: True\n",
    " \n",
    "Weather Station ID: 3\n",
    "Pollution_level\n",
    "  Measurement: Pollution_level, Extracted Mean: 0.23818965517241378, Original Mean: 0.27146439734566075, Within Spec: False\n",
    " \n",
    "Rainfall\n",
    "  Measurement: Rainfall, Extracted Mean: 905.1913970588236, Original Mean: 931.6078595317725, Within Spec: False\n",
    " \n",
    "Temperature\n",
    "  Measurement: Temperature, Extracted Mean: 13.256778523489933, Original Mean: 13.224665551839465, Within Spec: True\n",
    " \n",
    "Weather Station ID: 4\n",
    "Pollution_level\n",
    "  Measurement: Pollution_level, Extracted Mean: 0.1282608695652174, Original Mean: 0.13991297519041454, Within Spec: False\n",
    " \n",
    "Rainfall\n",
    "  Measurement: Rainfall, Extracted Mean: 1200.1835051546393, Original Mean: 1168.9918003565062, Within Spec: False\n",
    " \n",
    "Temperature\n",
    "  Measurement: Temperature, Extracted Mean: 13.188571428571429, Original Mean: 13.197994652406416, Within Spec: True\n",
    " \n",
    "True: 7, False: 8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c419a3a",
   "metadata": {},
   "source": [
    "Oh no... `True: 7, False: 8`, which means some of the values don't match. Does this mean our data are not a true reflection of reality?\n",
    "\n",
    "This is a bit of a disaster. I am going to think about this a bit, you do the same, and when we meet again we can share ideas.\n",
    "\n",
    "Until next time.\n",
    "\n",
    "Saana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd094b-0fee-46f1-a4b8-73766813c42b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
